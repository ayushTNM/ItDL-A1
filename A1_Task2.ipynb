{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDE DIRECTORY OF CLOCK IMAGES AND LABELS HERE (DATA NOT INCLUDED IN SUBMISSION)\n",
    "data_dir = './data/CLOCKS/'  \n",
    "\n",
    "# 0: GENERIC TRAINING METHOD AND CNN MODEL (GENERATES TABLE 1) CAN BE USED TO GENERATE ANY OF THE MODELS\n",
    "# 1: LOAD BEST CLASSIFICATION MODEL AND SHOW MISCLASSIFIED IMAGES (FIGURE 1)\n",
    "# 2: STATISTICS CLASSIFICATION (TABLE 2)\n",
    "# 3: LOAD BEST REGRESSION MODEL\n",
    "# 4: SHOW RESULTS REGRESSION (FIGURE 2) \n",
    "# 5: ANALYSE MULTIHEAD PREDICTIONS (FIGURE 3) \n",
    "# 6: LOAD BEST MULTICYCLIC MODEL \n",
    "# 7: ANALYSE BEST MULTICYCLIC MODEL (FIGURE 4)\n",
    "\n",
    "load_existing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 16:21:09.909934: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-09 16:21:09.936381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 16:21:09.936406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 16:21:09.936428: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-09 16:21:09.941775: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 16:21:10.507988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-09 16:21:11.405410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-09 16:21:11.454821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-09 16:21:11.454955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# 0: GENERIC TRAINING METHOD (TABLE 1) >> generate all models from here \n",
    "import os\n",
    "import csv\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import clockwork_model as cw\n",
    "import clockwork_utils as cu\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "if load_existing == False:\n",
    "    def main(num_kernels, learning_rate, depth, dropout, mode):\n",
    "        \n",
    "        random_seed = 42\n",
    "\n",
    "        # data parameters\n",
    "        num_images = 18000\n",
    "        train_split = 0.64   # 80% x 80%\n",
    "        val_split = 0.16     # 80% x 20%\n",
    "        test_split = 1 - train_split - val_split\n",
    "\n",
    "        #architecture\n",
    "        input_shape = (150, 150, 1)\n",
    "        n_categories = int (720 / 10)\n",
    "        num_kernels=num_kernels\n",
    "        depth=depth\n",
    "        dropout1=dropout\n",
    "        dropout2=dropout/2\n",
    "        learning_rate = learning_rate\n",
    "\n",
    "        # hyper parameters\n",
    "        mode = mode\n",
    "        batch_size = 32\n",
    "        epochs = 50\n",
    "        patience = 6\n",
    "\n",
    "        # locations of i/o\n",
    "        data_dir = './data/CLOCKS/'  \n",
    "        marker_name = f'{mode}_{num_kernels}_{depth}_{learning_rate:.2e}_{dropout1:.2e}:{batch_size}_{random_seed}'\n",
    "        best_model_dir = './best_model'\n",
    "        best_model_path = best_model_dir + f'/best_model_{marker_name}' \n",
    "        checkpoint_dir = f'./training/training_{marker_name}' \n",
    "\n",
    "        log_dir, checkpoint_path = cu.create_check_log_dir(checkpoint_dir, marker_name)\n",
    "\n",
    "        # calculate sizes of splits for later\n",
    "        num_train = int(train_split * num_images)\n",
    "        num_val = int(val_split * num_images)\n",
    "        num_test = num_images - num_train - num_val  \n",
    "        # class_names = [str(i) for i in range(n_categories)]\n",
    "\n",
    "        print(f\"loading from {data_dir}\")\n",
    "        train_dataset, val_dataset, test_dataset = cu.load_and_process_data(data_dir, num_train, num_val, batch_size, mode)\n",
    "\n",
    "        # [Model loading or creation] \n",
    "        model = cw.create_model(\n",
    "            mode=mode,\n",
    "            input_shape=input_shape, \n",
    "            n_categories=n_categories,\n",
    "            num_kernels=num_kernels,\n",
    "            depth=depth,\n",
    "            dropout1=dropout1,\n",
    "            dropout2=dropout2)\n",
    "\n",
    "        compiler_loss, compiler_metrics = cu.get_compiler(mode)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=compiler_loss,\n",
    "            metrics=compiler_metrics)  \n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        callbacks_list = cu.get_callbacks(model, train_dataset, log_dir, checkpoint_path, patience, mode)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,  # Use the validation set here\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            validation_steps=num_val // batch_size,  # steps for the validation set\n",
    "            verbose=1,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks_list)\n",
    "\n",
    "        # Load latest checkpoint file to best model location\n",
    "        best_model_file = cu.latest_checkpoint(checkpoint_dir)\n",
    "        best_model = load_model(best_model_file)\n",
    "        best_model.save(best_model_path)\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        \n",
    "        # settings to generate best classification model for 720 classes \n",
    "        mode_list=['classify', 'regress', 'multihead', 'cyclic']\n",
    "        mode = mode_list[0]\n",
    "        num_kernels_list=[(32, 64, 128, 256, 256)]\n",
    "        depth_list=[None]\n",
    "        lr_list = [1.e-4]\n",
    "        dropout_list = [0.02]\n",
    "        n_categories = [720]\n",
    "\n",
    "        log_file_path = f'./best_model/model_{mode}_logs.csv'  \n",
    "        header = ['index', 'nk_idx', 'd_idx', 'lr_idx', 'do_idx'] + [f'epoch_{i}' for i in range(1, 51)]  # max epochs = 50\n",
    "        with open(log_file_path, 'w', newline='') as file:  # Opening the file in write mode to create it/reset it\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)  # Writing the header\n",
    "        \n",
    "        for nk_idx, num_kernels in enumerate(num_kernels_list):\n",
    "            for d_idx, depth in enumerate(depth_list):\n",
    "                for lr_idx, lr in enumerate(lr_list):        \n",
    "                        for do_idx, dropout in enumerate(dropout_list):\n",
    "                            main(num_kernels, lr, depth, dropout, mode)\n",
    "                            print(f\"{mode} on {n_categories} classes : kernel = {num_kernels} depth = {depth}, learning rate = {lr}, dropout = {dropout}\")\n",
    "\n",
    "        # Store the data here\n",
    "        rows = []\n",
    "        with open(log_file_path, 'r') as csvfile:\n",
    "            # Create a CSV reader\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            for row in csvreader:\n",
    "                rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./best_model/best_model_classify_720_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
      "113/113 [==============================] - 5s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "# 1: LOAD BEST CLASSIFICATION MODEL AND SHOW MISCLASSIFIED IMAGES (FIGURE 1)\n",
    "\n",
    "# EXISTING DATA:\n",
    "# ./best_model/best_model_classify_720_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "# ./best_model/best_model_classify_360_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "# ./best_model/best_model_classify_240_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "# ./best_model/best_model_classify_180_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "# ./best_model/best_model_classify_144_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "# ./best_model/best_model_classify_120_(32, 64, 128, 256, 256)_None_1.00e-04_2.00e-02:32_42\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# data parameters\n",
    "mode='classify'\n",
    "num_images = 18000\n",
    "train_split = 0.64   # 80% x 80%\n",
    "val_split = 0.16     # 80% x 20%\n",
    "\n",
    "# Choices go here\n",
    "# number of classes for time-telling\n",
    "n_categories_list = [120, 144, 180, 240, 360, 720]\n",
    "# different models and labels, can run in batch or just select one model\n",
    "mode_list=['classify', 'regress', 'multihead', 'cyclic', 'multicyclic']\n",
    "# kernels for the 5 convolutional model\n",
    "num_kernels_list=[(32, 64, 128, 256, 256), (32, 32, 64, 64, 64)]\n",
    "# use of depth kernel\n",
    "depth_list=['None', 3, 4]\n",
    "# initial learning rate\n",
    "lr_list = [1.e-4, 2.e-4, 5.e-4]\n",
    "# intial drop-out\n",
    "dropout_list = [0., 1.e-2, 1.3e-2, 2.e-2, 1.e-1, 2.e-1, 5.e-1]\n",
    "\n",
    "# best model for classification goes here\n",
    "mode = mode_list[0]\n",
    "n_categories = n_categories_list[5]\n",
    "num_kernels = num_kernels_list[0]\n",
    "depth = depth_list[0]\n",
    "learning_rate = lr_list[0]\n",
    "drop_out = dropout_list[3]\n",
    "\n",
    "# architecture\n",
    "input_shape = (150, 150, 1)\n",
    "batch_size = 32\n",
    "\n",
    "# index to keep track if we run multiple models simultaneously\n",
    "global_index = 0  \n",
    "\n",
    "# locations of data\n",
    "data_dir = './data/CLOCKS/'  \n",
    "\n",
    "# run generic loader from here\n",
    "# calculate sizes of splits for later\n",
    "num_train = int(train_split * num_images)\n",
    "num_val = int(val_split * num_images)\n",
    "num_test = num_images - num_train - num_val  \n",
    "# class_names = [str(i) for i in range(n_categories)]\n",
    "\n",
    "class_names = [str(i) for i in range(n_categories)]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = cu.load_and_process_data(data_dir, num_train, num_val, batch_size, mode, n_categories, random_seed)\n",
    "\n",
    "best_model_file = f'./best_model/best_model_{mode}_{n_categories}_{num_kernels}_{depth}_{learning_rate:.2e}_{drop_out:.2e}:{batch_size}_{random_seed}' \n",
    "print(best_model_file)\n",
    "\n",
    "best_model = load_model(best_model_file)\n",
    "\n",
    "test_labels, predicted_labels, test_images = cu.extract_accuracy_prediction(test_dataset, best_model)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels, average='macro')  # 'macro' for unweighted mean\n",
    "precision = precision_score(test_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(test_labels, predicted_labels, average='macro')\n",
    "# print(f'{int(720/n_categories)} & {n_categories} & {100* accuracy:6.2f}\\% & {100*precision:6.2f}\\% & {100*recall:6.2f}\\% & {100*f1:6.2f}\\% \\\\\\\\')\n",
    "\n",
    "incorrect_labels = np.where(predicted_labels != test_labels)[0]\n",
    "# print(len(incorrect_labels))\n",
    "\n",
    "# Calculate the error distances for incorrect predictions\n",
    "error_distances = (predicted_labels[incorrect_labels] - test_labels[incorrect_labels])\n",
    "error_distances = np.abs(error_distances) % n_categories  # Ensure error wraps around for circular categories\n",
    "\n",
    "# Correct for wrap-around at the halfway point\n",
    "halfway_point = n_categories // 2\n",
    "error_distances = np.where(error_distances > halfway_point, n_categories - error_distances, error_distances)\n",
    "\n",
    "# Count the frequency of each error distance\n",
    "error_distance_counts = np.bincount(error_distances)\n",
    "\n",
    "# For presentation, let's create a dictionary that skips the 0 error (since we're only interested in incorrect ones)\n",
    "error_summary = {distance: count for distance, count in enumerate(error_distance_counts) if distance > 0 and count > 0}\n",
    "\n",
    "# print(error_summary)\n",
    "\n",
    "# Calculate parameters for time conversion\n",
    "m = int(720 / n_categories)\n",
    "n = int(60 / m)\n",
    "\n",
    "# Set up the plot\n",
    "num_incorrect = len(incorrect_labels)\n",
    "if num_incorrect > 0:\n",
    "    columns = 8\n",
    "    rows = (num_incorrect + columns - 1) // columns  # Ensure enough rows to show all incorrect images\n",
    "    plt.figure(figsize=(2 * columns, 2 * rows))\n",
    "\n",
    "    # Loop through the incorrect predictions\n",
    "    for plot_idx, index in enumerate(incorrect_labels):\n",
    "        image = test_images[index]\n",
    "        true_label_int = test_labels[index]\n",
    "        predicted_label_int = predicted_labels[index]\n",
    "\n",
    "        true_h = int(true_label_int / n)\n",
    "        true_m = int(true_label_int % n) * m\n",
    "        pred_h = int(predicted_label_int / n)\n",
    "        pred_m = int(predicted_label_int % n) * m\n",
    "\n",
    "        class_error = int((true_label_int - predicted_label_int + n_categories) % n_categories)\n",
    "        class_error = min(class_error, n_categories - class_error)\n",
    "\n",
    "        # Determine color based on the magnitude of the error\n",
    "        if class_error == 0:\n",
    "            color = 'green'\n",
    "        elif class_error <= m:\n",
    "            color = 'orange'\n",
    "        else:\n",
    "            color = 'red'\n",
    "\n",
    "        plt.subplot(rows, columns, plot_idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"True: {true_h:02d}:{true_m:02d}\\nPred: {pred_h:02d}:{pred_m:02d}\", color=color, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"misclassified test.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: STATISTICS CLASSIFICATION (TABLE 2)\n",
    "\n",
    "# print(test_labels)\n",
    "# print(predicted_labels)\n",
    "test_hour_labels = np.floor_divide(test_labels, 60)\n",
    "test_min_labels = np.mod(test_labels, 60)\n",
    "pred_hour_labels = np.floor_divide(predicted_labels, 60)\n",
    "pred_min_labels = np.mod(predicted_labels, 60)\n",
    "accuracy_hour = accuracy_score(test_hour_labels, pred_hour_labels)\n",
    "recall_hour = recall_score(test_hour_labels, pred_hour_labels, average='macro')  # 'macro' for unweighted mean\n",
    "precision_hour = precision_score(test_hour_labels, pred_hour_labels, average='macro')\n",
    "f1_hour = f1_score(test_hour_labels, pred_hour_labels, average='macro')\n",
    "accuracy_min = accuracy_score(test_min_labels, pred_min_labels)\n",
    "recall_min = recall_score(test_min_labels, pred_min_labels, average='macro')  # 'macro' for unweighted mean\n",
    "precision_min = precision_score(test_min_labels, pred_min_labels, average='macro')\n",
    "f1_min = f1_score(test_min_labels, pred_min_labels, average='macro')\n",
    "print(\"Latex output for report\")\n",
    "print(f'{int(720/n_categories)} & {n_categories} & {100* accuracy_hour:6.2f}\\% & {100*precision_hour:6.2f}\\% & {100*recall_hour:6.2f}\\% & {100*f1_hour:6.2f}\\% \\\\\\\\')\n",
    "print(f'{int(720/n_categories)} & {n_categories} & {100* accuracy_min:6.2f}\\% & {100*precision_min:6.2f}\\% & {100*recall_min:6.2f}\\% & {100*f1_min:6.2f}\\% \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: LOAD BEST REGRESSION MODEL\n",
    "\n",
    "# EXISTING DATA:\n",
    "# ./best_model/best_model_regress_720_(32, 64, 128, 256, 256)_None_1.00e-04_1.30e-02:32_42\n",
    "\n",
    "\n",
    "if load_existing == True:\n",
    "\n",
    "    # Choices go here\n",
    "    # number of classes for time-telling\n",
    "    n_categories_list = [120, 144, 180, 240, 360, 720]\n",
    "    # different models and labels, can run in batch or just select one model\n",
    "    mode_list=['classify', 'regress', 'multihead', 'cyclic', 'multicyclic']\n",
    "    # kernels for the 5 convolutional model\n",
    "    num_kernels_list=[(32, 64, 128, 256, 256), (32, 32, 64, 64, 64)]\n",
    "    # use of depth kernel\n",
    "    depth_list=['None', 3, 4]\n",
    "    # initial learning rate\n",
    "    lr_list = [1.e-4, 2.e-4, 5.e-4]\n",
    "    # intial drop-out\n",
    "    dropout_list = [0., 1.e-2, 1.3e-2, 2.e-2, 1.e-1, 2.e-1, 5.e-1]\n",
    "\n",
    "    # best model for classification goes here\n",
    "    mode = mode_list[1]\n",
    "    n_categories = n_categories_list[4]\n",
    "    num_kernels = num_kernels_list[0]\n",
    "    depth = depth_list[0]\n",
    "    learning_rate = lr_list[0]\n",
    "    drop_out = dropout_list[2]\n",
    "\n",
    "    random_seed = 42\n",
    "    mode = 'regress'\n",
    "    num_images = 18000\n",
    "    train_split = 0.64   # 80% x 80%\n",
    "    val_split = 0.16     # 80% x 20%\n",
    "\n",
    "    #architecture\n",
    "    input_shape = (150, 150, 1)\n",
    "    n_categories = 720\n",
    "    batch_size = 32\n",
    "    class_names = [str(i) for i in range(n_categories)]\n",
    "\n",
    "    # locations of i/o\n",
    "    data_dir = './data/CLOCKS/'  \n",
    "    best_model_file = f'./best_model/best_model_{mode}_{n_categories}_{num_kernels}_{depth}_{learning_rate:.2e}_{drop_out:.2e}:{batch_size}_{random_seed}' \n",
    "    print(best_model_file)\n",
    "\n",
    "    # calculate sizes of splits for later\n",
    "    num_train = int(train_split * num_images)\n",
    "    num_val = int(val_split * num_images)\n",
    "    num_test = num_images - num_train - num_val  \n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = cu.load_and_process_data(data_dir, num_train, num_val, batch_size, mode, n_categories, random_seed)\n",
    "    best_model = load_model(best_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: SHOW RESULTS REGRESSION (FIGURE 2)\n",
    "\n",
    "def calculate_r_squared(actual, predicted):\n",
    "    # Calculate the mean of the actual values\n",
    "    mean_actual = np.mean(actual)\n",
    "\n",
    "    # Calculate total sum of squares\n",
    "    total_sum_of_squares = np.sum((actual - mean_actual) ** 2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_sum_of_squares = np.sum((actual - predicted) ** 2)\n",
    "\n",
    "    # Calculate the R² score\n",
    "    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "\n",
    "    return r_squared\n",
    "\n",
    "def calculate_residual_statistics(actual, predicted):\n",
    "    residuals = actual - predicted\n",
    "    std_residuals = np.std(residuals)\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    mse = np.mean(residuals**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return residuals, std_residuals, mae, mse, rmse\n",
    "\n",
    "def plot_regression_results(model, dataset, dataset_name=\"\"):\n",
    "\n",
    "    # First, concatenate the batches in the dataset into one large batch\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_batch, label_batch in dataset:\n",
    "        images.append(image_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    # Stack the batches (now the entire dataset is in memory)\n",
    "    images = tf.concat(images, axis=0)\n",
    "    labels = tf.concat(labels, axis=0)\n",
    "\n",
    "    # Get the actual and predicted values\n",
    "    predictions = 720 * model.predict(images).squeeze()\n",
    "    actual_values = 720 * labels.numpy()  # Convert to numpy array\n",
    "\n",
    "    r_squared = calculate_r_squared(actual_values, predictions)\n",
    "    residuals, std_residuals, mae, mse, rmse = calculate_residual_statistics(actual_values, predictions)\n",
    "   \n",
    "    print(\"regression R² score:\", r_squared)\n",
    "    print(f\"Standard Deviation of Residuals: {std_residuals}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    # Assuming 'predictions' are the continuous outputs from your regression model\n",
    "    predicted_classes = np.rint(720 * predictions).astype(int)\n",
    "    print(predicted_classes)\n",
    "\n",
    "    # Start a new figure\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Create the first subplot for the regression scatter plot\n",
    "    plt.subplot(1, 2, 1)  # (rows, columns, panel number)\n",
    "    plt.scatter(actual_values, predictions, alpha=0.6)  # alpha for transparency\n",
    "\n",
    "    # Add a line for perfect correlation. This is where actual == predicted\n",
    "    max_val = max(actual_values.max(), predictions.max())\n",
    "    min_val = min(actual_values.min(), predictions.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)  # 'k--' is style for black dashed line\n",
    "    plt.text(min_val+50, max_val-50, f'R² = {r_squared:.2f}', fontsize=14, fontweight='bold', verticalalignment='top', horizontalalignment='left')\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(f'{dataset_name} Data: Actual vs. Predicted', fontsize=14, fontweight='bold')\n",
    "    plt.xlim([0,720])\n",
    "    plt.ylim([0,720])\n",
    "    plt.xlabel('Actual Values', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Predicted Values', fontsize=12, fontweight='bold')\n",
    "    plt.axvline(x=0, color='k', linestyle='--')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    # Create the second subplot for the histogram of residuals\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bin_width = 2  # bin width set to 2 minutes\n",
    "    min_residual = np.floor(min(residuals))\n",
    "    max_residual = np.ceil(max(residuals))\n",
    "\n",
    "    # Create bins from min to max with a step of 2 minutes\n",
    "    bins = np.arange(min_residual, max_residual + bin_width, bin_width)\n",
    "    count, bins, ignored = plt.hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black', zorder=3)\n",
    "    max_count = 50 * (1 + int(max(count)/50))\n",
    "    # Draw fill_between behind the histogram by setting a lower zorder\n",
    "    plt.fill_betweenx([0, max_count], -2*std_residuals, 2*std_residuals, color='green', alpha=0.2, zorder=1)\n",
    "    plt.fill_betweenx([0, max_count], -std_residuals, std_residuals, color='yellow', alpha=0.4, zorder=2)\n",
    "    plt.title(f'{dataset_name} Data: Histogram of Residuals', fontsize=14, fontweight='bold', )\n",
    "    plt.xlabel('Residuals (minutes)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    plt.ylim([0,max_count])\n",
    "    plt.axvline(x=0, color='k', linestyle='--')  # Add a vertical line at x=0 for reference\n",
    "    plt.text(bins[0], max_count - 20, f'Std Dev = {std_residuals:.2f}\\nMAE = {mae:.2f}', fontsize=14, fontweight='bold', verticalalignment='top', horizontalalignment='left')\n",
    "    # Shading 1 and 2 standard deviation\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('regression_result.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# plot the regression results for the test dataset\n",
    "plot_regression_results(best_model, test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: ANALYSE MULTIHEAD PREDICTIONS (FIGURE 3)\n",
    "# './logs/logmultihead_720_(32, 64, 128, 256, 256)_None_1.00e-04_1.00e-01:32_42_log.csv'\n",
    "\n",
    "if load_existing == True:\n",
    "\n",
    "    # Read the CSV files into pandas DataFrames\n",
    "    df1 = pd.read_csv('./logs/logmultihead_720_(32, 64, 128, 256, 256)_None_1.00e-04_1.00e-01:32_42_log.csv', sep=';')\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))  # 2 rows, 1 column for subplots\n",
    "\n",
    "    # First subplot for loss\n",
    "    axs[0].plot(df1['epoch'], df1['val_h_loss'], label='hour validation loss', c='blue', linestyle='-')  \n",
    "    axs[0].plot(df1['epoch'], df1['val_m_loss'], label='minute validation loss', c='red', linestyle='-')\n",
    "\n",
    "    # Customize the first subplot\n",
    "    axs[0].set_title('Learning curve validation loss', fontsize=18, fontweight='bold', )\n",
    "    axs[0].set_xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    axs[0].set_ylabel('Loss', fontsize=14, fontweight='bold', )\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[0].set_ylim([1.e-4, 10])\n",
    "    axs[0].set_xlim([0, 150])\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=14)  \n",
    "    axs[0].legend(fontsize='large')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Second subplot for error\n",
    "    hour_epoch = 110\n",
    "    minute_epoch = 139\n",
    "    axs[1].plot(100 * (1 - df1['val_h_accuracy'][:hour_epoch]), c='blue', label='hour error', linestyle='-')  \n",
    "    axs[1].plot(100 * (1 - df1['val_m_accuracy'][:minute_epoch]), c='red', label='minute error', linestyle='--')  \n",
    "\n",
    "    # Customize the second subplot\n",
    "    axs[1].set_title('Learning curve classification error', fontsize=18, fontweight='bold')\n",
    "    axs[1].set_xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    axs[1].set_ylabel('Error (%)', fontsize=14, fontweight='bold')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_ylim([1.e-2, 100])\n",
    "    axs[1].set_xlim([0, 150])\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=14)  \n",
    "    axs[1].legend(fontsize='large')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the combined figure as a PDF\n",
    "    plt.savefig('multihead_combined.pdf')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: LOAD BEST MULTICYCLIC MODEL \n",
    "\n",
    "# EXISTING DATA\n",
    "# './best_model/best_model_multicyclic_720_(32, 64, 128, 256, 256)_None_1.00e-04_1.00e-01:32_42/'\n",
    "\n",
    "\n",
    "if load_existing == True:\n",
    "\n",
    "    random_seed = 42\n",
    "\n",
    "    # data parameters\n",
    "    mode='multicyclic'\n",
    "    num_images = 18000\n",
    "    train_split = 0.64   # 80% x 80%\n",
    "    val_split = 0.16     # 80% x 20%\n",
    "\n",
    "    # n_categories_list = [120, 144, 180, 240, 360, 720] \n",
    "    n_categories = 720 \n",
    "\n",
    "    #architecture\n",
    "    input_shape = (150, 150, 1)\n",
    "    # n_categories = n_categories_list[4]\n",
    "    batch_size = 32\n",
    "\n",
    "    # locations of i/o\n",
    "    data_dir = './data/CLOCKS/'  \n",
    "\n",
    "    # calculate sizes of splits for later\n",
    "    num_train = int(train_split * num_images)\n",
    "    num_val = int(val_split * num_images)\n",
    "    num_test = num_images - num_train - num_val  \n",
    "    class_names = [str(i) for i in range(n_categories)]\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = cu.load_and_process_data(data_dir, num_train, num_val, batch_size, mode, n_categories, random_seed)\n",
    "\n",
    "    best_model = load_model(f'./best_model/best_model_multicyclic_720_(32, 64, 128, 256, 256)_None_1.00e-04_1.00e-01:32_42/')\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    for images, (labels_h, labels_sm, labels_cm) in test_dataset:\n",
    "        test_images.append(images.numpy())\n",
    "        stacked_labels = np.stack([labels_h.numpy(), labels_sm.numpy(), labels_cm.numpy()], axis=0)    \n",
    "        test_labels.append(stacked_labels)\n",
    "\n",
    "    # Once all batches are processed, concatenate them into a single 3 x test_size array\n",
    "    test_labels = np.concatenate(test_labels, axis=1)\n",
    "\n",
    "    # Converting list to numpy array for computation\n",
    "    test_images = np.concatenate(test_images)\n",
    "\n",
    "    pred_labels = best_model.predict(test_images)\n",
    "\n",
    "    classification_h = np.argmax(pred_labels[0], axis=-1)\n",
    "    regression_sm = pred_labels[1].squeeze()  # Remove the singleton dimension\n",
    "    regression_cm = pred_labels[2].squeeze()  # Remove the singleton dimension\n",
    "\n",
    "    # Now, all the arrays to be stacked are 1D\n",
    "    pred_labels = np.stack((classification_h, regression_sm, regression_cm), axis=-1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: ANALYSE BEST MULTICYCLIC MODEL \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# convert back to original [h, m] format\n",
    "# Calculate the angle in radians for hours and minutes from the sine and cosine values\n",
    "def angles_to_time(angle_labels):\n",
    "\n",
    "    hours = angle_labels[0, :].astype(int)\n",
    "    minutes_angle = np.arctan2(angle_labels[1, :], angle_labels[2, :])\n",
    "\n",
    "    minutes = (minutes_angle * (60 / (2 * np.pi)))  # Convert radian to minute\n",
    "\n",
    "    # Rounding to the nearest integer\n",
    "    minutes = np.rint(minutes).astype(int)\n",
    "\n",
    "    # and make sure we stay in the circle\n",
    "    minutes = np.mod(np.rint(minutes).astype(int),60).squeeze()\n",
    "    \n",
    "    time_label = np.stack((hours, minutes))\n",
    "\n",
    "    return time_label\n",
    "\n",
    "def calculate_r_squared(actual, predicted):\n",
    "    # Calculate the mean of the actual values\n",
    "    mean_actual = np.mean(actual)\n",
    "\n",
    "    # Calculate total sum of squares\n",
    "    total_sum_of_squares = np.sum((actual - mean_actual) ** 2)\n",
    "\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_sum_of_squares = np.sum((actual - predicted) ** 2)\n",
    "\n",
    "    # Calculate the R² score\n",
    "    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "\n",
    "    return r_squared\n",
    "\n",
    "def calculate_residual_statistics(actual, predicted):\n",
    "    residuals = actual - predicted\n",
    "    std_residuals = np.std(residuals)\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    mse = np.mean(residuals**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return residuals, std_residuals, mae, mse, rmse\n",
    "\n",
    "# convert from (h, sin, cos) => (h, m)\n",
    "pred_labels_orig = angles_to_time(pred_labels)\n",
    "test_labels_orig = angles_to_time(test_labels)\n",
    "\n",
    "# convert from (h, m) => x where x is now a continuous regression variable\n",
    "pred_labels_line = 60 * pred_labels_orig[0,:] + pred_labels_orig[1,:]\n",
    "test_labels_line = 60 * test_labels_orig[0,:] + test_labels_orig[1,:]\n",
    "\n",
    "differences = np.abs(test_labels_line - pred_labels_line)\n",
    "\n",
    "# # Use np.bincount to count occurrences of each difference\n",
    "# # The result will be an array where the index is the difference and the value is the count\n",
    "difference_counts = np.bincount(differences)\n",
    "print(difference_counts)\n",
    "\n",
    "r_squared = calculate_r_squared(test_labels_line, pred_labels_line)\n",
    "residuals, std_residuals, mae, mse, rmse = calculate_residual_statistics(test_labels_line, pred_labels_line)\n",
    "\n",
    "print(\"regression R² score:\", r_squared)\n",
    "print(f\"Standard Deviation of Residuals: {std_residuals}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
